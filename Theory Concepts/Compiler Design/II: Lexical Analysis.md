# Lexical Analysis

Lexical analysers can be produced either from scratch, or specifying lexeme patterns to a lexical-analyser generator and compiling those patterns into code that functions as a lexical analyser.

A lexical-analyser generator allows to only rewrite affected patterns, and speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level.

## Role of the Lexical Analyser

The main task of the lexical analyser is to rad input characters of a source program and group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program.

When the lex discovers a lexeme constituting an identifier, it enters the lexeme into the symbol table. In some cases, this information can be read from the sumbol table into the lex.

The lex also performs the tasks of stripping out comments and whitespaces. It also helps correlated error messages generated by the compiler.

This lexing is divided into 
1. Scanning - deleting comments and whitespaces
2. Lexical analysis - token generation

### Lexical Analysis and Parsing

1. The separation of lexical and syntacrical analysis allows to simplify atleast one of these tasks
2. Compiler efficiency is improved
3. Compiler portability is enhanced, and input device specifics are restricted to the lexer.

### Tokens, Patterns and Lexemes

1. A token is a pair consisting of a token name and an optional attribute.
   1. A token name is an abstract symbol representing a lexical unit.
2. A pattern is a description of the form that lexemes of a token may take.
3. A lexeme is a sequence of characters from the source that match a pattern for some token.

### Attributes for Tokens

Attributes specify additional information about a token being generated to the parser. The attribute values incluence translation of tokens after the parse.

### Lexical Errors

Lexers are unable to proceed due to compiler errors in the code that do not match any lexemes. In this case, the lexer performs a panic mode recovery, where successive characters are deleted from the remaining input, until a well formed token is found.
Other recovery actions may be
1. Delete one character from the remaining input
2. Insert a missing character into the remaining input
3. Replace a character by another
4. Transpose 2 adjacent characters.

## Input Buffering

Input buffering is used to speed up the reading of the source program, with the help of buffer pairs and sentinels.

### Buffer Pairs

This is done to reduce the amount of overhead required to process a single input character. 

Instead of reading one character per system call, we read N characters in the buffer. If fewer than N characters remain in the input, a special eof character marks the end of the source file.

Two pointers to the input are maintained
1. lexemeBegin marks the beginning of the current lexeme
2. forward scans ahead until a pattern match is found.

Forward is advanced if the end of one of the buffers is reached, where forward is moved to the beginning of the newly loaded buffer. 

### Sentinels

In buffer pairs, we have to check each time we advance forward, to see if we have not moved off one of the buffers, and if we do, another buffer must be reloaded.

This buffer end test can be combined with the test for the current character if a sentinel character is maintained at the end of each buffer, as eof.

## Specification of Tokens

1. Aphabets
   1. Letters, digits, punctuation
   2. Finite set of symbols
   3. {0, 1} is the binary alphabet
   4. A string is a finite sequence of symbols drawn over an alphabet
   5. The language is any countable set of strings over some fixed alphabet
2. Operations
   1. Union
   2. Concatenation
   3. Kleene Closure(L*) = Set of strings got by concatenating L zero or more times
   4. Positive Closure (L+) = Kleene closure without L0.
3. Regex
   1. Each regex r denotes a language L(r), which can be defined from r's subexpressions.
   2. Rules that define regexs are
      1. Basis 
         1. E is a regular expression, and L(E) is {E}
         2. If a is a symbol in the alphabet, then <b>a</b> is a regex and L(a) = {a}.
      2. Induction
         1. (r)|(s) denotes L(r) U L(s)
         2. (r)(s) denotes L(r)L(s)
         3. (r)* denotes (L(r))*
         4. (r) denotes L(r)
      3. Some paranthesis may be dropped if the conventions are
         1. The unary operator * has the highest precedence and is left associative
         2. Concatenation has second highest precedence and is left associative
         3. | has lowest precedence and is left associative.
4. Regular Definitions
   1. Some regexs are given names, called definitions where d -> r.
   2. Each d is a new unique symbol and each r is a regex over the alphabet

## Recognition of Tokens - Transition Diagrams

Patterns must be first converted to transition diagrams, representing these regex patterns. They are collections of states, each state representing a testing condition.

The difference from traditional transition diagrams is the ability to retract the forward moving pointer one position, which is done by placing another * at an accepting state.

Large codebases can fit in transition diagrams with the following techniques
1. Arrange for TDs for each token to be tried sequentially
2. Run all TDs in parallel, while handling multiple matches
3. (Preferred) Combine all TDs into one.

## The Lexical Analyzer Generator - Lex

The lex compiler transforms input patterns to a TD and generates code in a file called lex.yy.c, that simulates the TD.

### The Use of Lex

The lex compiler transforms lex.l to a C program, in a file called lex.yy.c. This is compiled by the C compiler to an object file, which helps transform a stream into tokens.

a.out is used as the subroutine of the parser, where it returns an integer as the code for one of the possible token names. The attribute value, whether it be another numeric code or pointer, is placed in yylval, which is shared between the lexical analyser and parser.

### Structure of Lex

The lex program has the following structure
```
declarations
%%
translation rules
%%
auxillary functions
```

The declarations section consists of variable and manifest constant declarations, and regular definitions.

The translation rules each have the form Pattern {Action}.

The third section holds additional functions used in the actions, which can be compiled seperately and loaded with the lexical analyser.

In the declarations section, anything within
```
%{
    code
$}
```
is not treated as a regular definition, and is copied directly into the lex.yy.c file. #define statements are used to define manifest constants.

The declarations section also has a sequence of regular definitions, for extended notations in the regexs.

Everything in the auxillary section is also copied directly to the lex.yy.c file.

### Conflict Resolution in Lex

The two main rules used in conflict resolution in lex are
1. Always prefer the longest prefix that matches
2. If the longest prefix matches 2 or more patterns, prefer the pattern that comes first

### The Lookahead Operator

/ allows additional patterns to be matched for complex tokens.

## Design of the Lex

The lex program is turned into a TD, used by a DFA simulator, whose components are
1. Transition table for automaton
2. Functions passed directly to lex
3. Actions from the input program to be invoked by the automaton sim.

The NFA constructed first has null transitions to all the different regexs, and this is built out to be a proper NFA when expanded. This NFA is then reduced to a DFA, via subset construction.

The lookahead operator / is treated as null.
# Deep Sequence Modelling

- Problem with naive NNs is due to fixed input sizes
  - Use fixed windows, say of length 2
  - For sequence modeling, we can use encoding to describe the input
  - But this gives a very limited history, and cannot model long term dependencies
  - Use entire sequences as a set of counts
    - This is called a  bag of words
    - Counts don't preserve order, so it doesnt work, for say, sentences
  - Use a really big fixed window
    - No parameter sharing, so dependence won't be captured

- To model sequences, we need to
  - Handle variable length sequences
  - Track long term dependencies
  - Maintain order
  - Share parameters across the sequence

- Recurrent Neural Networks
  - RNNs contain loops, allowing information to persist over time
  - Internal states are updated in the hidden layers, called recurrent layers
  - We use a recurrence relation as h<sub>t</sub> = f<sub>w</sub>(h<sub>t-1</sub>, x<sub>t</sub>)
    - Here, h is the cell state, x is the input at time t and f is a function parameterised by the weight w
  - We reuse the same matrices
  - Loss is just summed over all timesteps

- Backpropogation through time
  - Backpropogation happens to previous timestamps as well
  - Repeated gradient computation occurs for the weight matrix and the derivative of the activation
    - Causes exploding gradients
    - Gradients are too large, and need to be scaled, or clipped
    - Can also cause vanishing gradients
    - Gradients are too small

- Problems of Long term dependencies
  - Vanishing Gradients
    - Very very small numbers multiplied
    - Errors due to further back time steps have smaller gradients
    - Bias parameters to capture short term dependencies
  - Standard RNNs cannot handle this
  - To alleviate
    - Choose a good activation function, with derivatives scale when x > 0
      - ReLU - Rectified Linear Unit
    - Parameter Initialization
      - Initialise weights to identity matrix
    - Use Gated Cells
      - Control what information is passed through, and what is used to update the cell
        - LSTM
        - GRU

- Long Short Term Memory (LSTM) Networks
  - Standard RNNs have repeating modules containing a simple computation node
  - LSTMs contain computational blocks that control information flow
    - Can track information through many timesteps
  - Information is added or removed through gates
  - Gates have a sigmoid neural net layer followed by a pointwise multiplication
  - This gates the flow of information
  - LSTM working
    - Forget
      - Forget irrelevant parts of the previous state
    - Store
      - Store relevant new information into the cell state
    - Update
      - Selectively update cell state values
    - Output
      - Controls what information is sent to the next timestep
  - All gating allows uninterrupted gradient flow

- Attention
  - Instead of decoders having only final state, they can use all timestamps
  - Provides learnable memory access
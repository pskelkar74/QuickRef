# Introduction to Deep Learning

- Deep learning is to learn underlying features directly from data.
  - Hand engineered features are time consuming and not scalable.

- Perceptrons
  - Structural building block of a neural network
  - Forward propogation
    - The output is obtained by a linear combination of inputs fed to a non linear activation function
    - A bias is applied to the linear combination to shift the activation to introduce a skew
    - y=g(w<sub>0</sub> + X<sup>T</sup>W)
      - Where g is the activation function, X is the input, W is the weight vector and w<sub>0</sub> is the bias
  - Some common activation functions
    - Sigmoid
    - Hyperbolic tangent
  - Activation functions
    - Introduce non-linearities into the network
    - Decision spaces should be non-linear by approximations

- Building Neural Networks
  - Multi output perceptrons
    - When all inputs are densely connected to every neuron in the output layer, these are called Dense layers.
  - Multiple hidden layers can be defined with the same idea.
  - Deep Neural Networks are Networks with more and more hidden layers stacked on top of each other.

- Applying Neural Networks
  - In the training set, we tell the network whats right and whats wrong
    - Apply a Cost function to aggregate all individual losses and average it for an input
  - Cross entropy loss is governed by how two curves generated for predicted and actual outputs differ
  - Mean squared error loss outputs continuous real numbers that averages the difference squared between the actual and predicted output.

- Training Neural Networks
  - We want the network weights that achieves the lowest loss
    - Take the weight vector that minimise the loss function J(W).
  - The loss function is just a function of the network weights
  - We compute the gradient of the function wrt of all the weights and go towards a downhill motion, till a local minima is found
  - This is called gradient descent
    - Initialise random weights
    - Loop until convergence
      - Compute gradient
      - Update weights towards downhill motion
    - Return weights
  - Backpropogation allows to find the direction to go in
    - How does a small change in one weight affect the final loss
    - Backprop all gradients from output to input and summarize all this

- Optimization
  - Training NNs is difficult
  - The learning rate determines the amount of steps we take in a direction
  - Small learning rate converges slowly and can get stuck in a false minima
  - Large learning rates can diverge and overshoot
  - The way to set it is
    - Trial and error
    - Adaptive learning rate
      - Not fixed
      - Make larger or smaller depending on a lot of factors
    - Examples
      - SGD
      - Adam
      - Adadelta
      - Adagrad
      - RMSProp

- Mini-batching
  - We perform batched gradient descent to reduce computation
  - We pick random points and calculate the descent
    - Can be noisy
    - Called Stochastic Gradient Descent
  - We take a mini-batch instead to give an affordable tradeoff
  - Faster and more accurate than Stochastic gradient descent
    - Allows larger learning rate
    - Parallelised computation

- Overfitting
  - Underfitting - Model can't learn the whole data
  - Overfitting - Too complex, doesn't generalise
  - We need something in between
  - Regularisation
    - Discourages complex models
    - Dropout
      - During training, randomly set some activations to 0
      - Dropout 50% of the activations
      - Forces network to not rely on any 1 node
    - Early Stopping
      - Stop training before we have a chance to overfit
      - Stop when testing data diverges from training data
      - Inflection point defines underfitting and overfitting boundaries
# Convolutional Neural Networks

- Tasks in Computer Vision
  - Regression
    - Output variable takes continuous value
  - Classification
    - Output variable is a class label

- Feature detection
  - Identify features in each image category
  - Manually, this is done by
    - Domain knowledge
    - Defining features
    - Detect features to classify
  - We instead, learn feature representations
    - A heirarchy of features

- Learning Visual Features
  - We use fully connected layers
  - We take the input 2D image as a vector of pixel values and pass it to the input layer
  - Instead of this, we can use a spatial structure, due to the loss of information
  - We represent the 2D image as an array of pixel values
  - We connect patches of input to neurons in hidden layers
    - Neuron connected to a region of input
  - We use a sliding window to associate patches to neurons and weight each patch
  - This is called convolution
    - Apply a set of weights, a filter to extract local features
    - Use multiple filters to extract different features
    - Spatially share parameters of each filter
  
- Feature Extraction and Convolution
  - If two objects approximately share features, they can be termed the same image
  - Convolution
    - Patch placed on top of input image
    - Element wise multiplication
    - Add outputs
  - The output matrix is called a feature map

- Convolutional Neural Networks
  - Convolutional layers
    - Convolution - Filter to generate feature maps
    - Non linearity - Often ReLU
    - Pooling - Downsampling operation on each feature map
  - Convolution
    - For a neuron in hidden layer
      - Take inputs
      - Compute weighted sum
      - Apply bias
    - Apply a window of weights
    - Compute a linear combination
    - Activate with a non-linear function
    - Slide patch over the input
  - Spatial arrangement of output volume
    - Output of Convolution is a volume
    - A stride is the filter step size
    - Receptive fields are locations in the input image that a node is path connected to
  - Non-linearity
    - Apply after each convolution operation
    - ReLU - replaces all negative to 0
  - Pooling
    - Reduce dimensionality
    - Spatial invariance
  - MaxPooling
    - Take maximum value of each patch
    - Reduces dimensionality much better
  - We can feed the output features to a dense or fully connected layer to predict classes, using softmax
# Deep Generative Models

- Take input training and learn a model that represents that distribution
  - Density estimation
    - Describe the sample's distribution
  - Sample Generation
    - Generate new samples to model training samples
    - Mainly done for debiasing data and detect outliers

- Latent Variable Models
  - Latent variables are explanatory factors that cannot be observed
  - Models try to detect these variables

- Autoencoders
  - Learning a lower dimensional feature representation from unlabeled data
  - Encoder learns mapping of the data x to a low-dimensional latent space z, so its a compressed representation of the data
  - We train a model to reconstruct the original data
  - Decoder learns mapping back from latent z, to reconstructed observations x'.
  - Minimize error between x' and x.
  - The lower the dimensionality of the latent space, the worse the quality of the reconstructed image
  - Autoencoders hence use a bottleneck hidden layer forcing a network to learn a compressed latent representation

- Variational Autoencoders
  - Used for sampling new input sets
  - Instead of a deterministic layer z, we use a stochastic layer
  - We keep a mean and stddev to describe the probability distribution of the latent variables
  - This allows variables belonging to simulated probability distributions based on these latent variables
  - The loss function becomes the reconstruction loss + a regularisation term
    - The regularisation allows constraints on the distribution
    - Maybe some initial guess of what the input distribution is
    - Causes less overfitting
  - We use a Gaussian curve to distribute the probability distribution, allowing less memorization
  - We cannot backprop gradients through sampling layers, as they're non deterministic
  - Here, we use reparametrizing the sample layer
    - Instead of drawing z from a Normal distribution, we consider z as a fixed vector mu and a fixed vector sigma, scaled by random constants drawn from a prior distribution
    - Sampling operation gets diverted, making z deterministic
  - Latent perturbation
    - Slowly increase or decrease a single latent variable
    - Allows different generated input being similar in nature
    - We want latent variabes to be uncorrelated with each other (Disentaglement)